{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "global_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v__Vb4YjQsJn"
      },
      "source": [
        "# imports \n",
        "import tarfile\n",
        "from os import path, chdir\n",
        "\n",
        "# constants \n",
        "working_directory = \"/content/drive/My Drive/train_global_model/\"\n",
        "\n",
        "# setup environment\n",
        "chdir(working_directory)\n",
        "\n",
        "# extract tar file of svg's\n",
        "fname = './assets/kanji_modified.tar.gz'\n",
        "\n",
        "if not path.isdir('./assets/kanji_modified'):\n",
        "  print('kanji modified svgs not found !, extracting ...')\n",
        "  tar = tarfile.open(fname, \"r:gz\")\n",
        "  tar.extractall(path=\"./assets/\")\n",
        "  tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdrZfP_iRNbS"
      },
      "source": [
        "from global_strokegenerator import *\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, filelist, batch_size, total_samples):\n",
        "        self.filelist = filelist\n",
        "        self.batch_size = batch_size\n",
        "        self.total_samples = total_samples\n",
        "        self.sg = strokeGenerator(self.filelist) # generator which yields x,y\n",
        "\n",
        "    def __len__(self):\n",
        "        # return steps per epoch\n",
        "        return self.total_samples // batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_batch = []\n",
        "        out_batch = []\n",
        "        # return ith step batch with len of dimenstion '0' = batch size\n",
        "        for batch in range(batch_size):\n",
        "          inp, out = next(self.sg)\n",
        "          inp_batch.append(inp)\n",
        "          out_batch.append(out) # predict out of 10,000 classes\n",
        "        return np.array(inp_batch), np.array(out_batch)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # get a new genrator to ensure same set of samples every epoch\n",
        "        self.sg = strokeGenerator(self.filelist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRY1bFBCW_ED"
      },
      "source": [
        "batch_size = 128\n",
        "inp_img_dim = [100, 100, 4]\n",
        "target_img_dim = 10000\n",
        "epochs = 10\n",
        "\n",
        "from os import walk\n",
        "path = \"./assets/kanji_modified/\"\n",
        "_, _, filelist = next(walk(path))\n",
        "\n",
        "train_samples = 40000\n",
        "train_files = filelist\n",
        "\n",
        "validation_samples = 5000\n",
        "validation_files = filelist[::-1]# get samples from back of file list\n",
        "\n",
        "train_data = DataGenerator(train_files, batch_size, train_samples)\n",
        "validation_data = DataGenerator(validation_files, batch_size, validation_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6UBr4rIVx3-"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# tf.compat.v1.experimental.output_all_intermediates(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BIV2OChXfO9"
      },
      "source": [
        "# define layout of res modules for weight initialization\n",
        "inp = Input(shape=(100, 100, 16))\n",
        "x = BatchNormalization()(inp)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(16, 3,padding=\"same\")(x) \n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(16, 3,padding=\"same\")(x)\n",
        "out = add([x, inp])\n",
        "res_block_16 = Model(inputs=inp, outputs=out)\n",
        "\n",
        "inp = Input(shape=(100, 100, 16))\n",
        "x = BatchNormalization()(inp)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(16, 3,padding=\"same\")(x) \n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(16, 3,padding=\"same\")(x)\n",
        "out = add([x, inp])\n",
        "res_block_16_1 = Model(inputs=inp, outputs=out)\n",
        "\n",
        "inp = Input(shape=(100, 100, 16))\n",
        "x = BatchNormalization()(inp)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(16, 3,padding=\"same\")(x) \n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(32, 3,padding=\"same\")(x)\n",
        "if not inp.shape[-1] == 32:\n",
        "  #project with 1x1 convolution\n",
        "  con = Conv2D(32,1)(inp)\n",
        "out = add([x, con])\n",
        "res_block_32 = Model(inputs=inp, outputs=out)\n",
        "\n",
        "inp = Input(shape=(100, 100, 32))\n",
        "x = BatchNormalization()(inp)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(16, 3,padding=\"same\")(x) \n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Conv2D(64, 3,padding=\"same\")(x)\n",
        "if not inp.shape[-1] == 64:\n",
        "  #project with 1x1 convolution\n",
        "  con = Conv2D(64,1)(inp)\n",
        "out = add([x, con])\n",
        "res_block_64 = Model(inputs=inp, outputs=out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsKUVc0tSlQm",
        "outputId": "e354cb86-dd48-4c10-aa93-b306522e46dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# input to global model\n",
        "inp = Input(shape=(inp_img_dim))\n",
        "\n",
        "# initilization layer to local mddel\n",
        "conv = Conv2D(16, 3, padding='same')(inp)\n",
        "#four residual block stacked \n",
        "x_a = res_block_16(conv) \n",
        "x_a = res_block_16_1(x_a) \n",
        "x_a = res_block_32(x_a) \n",
        "x_a = res_block_64(x_a)\n",
        "# reduce parameters \n",
        "x_a = MaxPooling2D(7)(x_a)\n",
        "\n",
        "x_a = Flatten()(x_a)\n",
        "\n",
        "x_a = Dense(1024, activation='relu')(x_a)\n",
        "\n",
        "out = Dense(target_img_dim, activation='softmax')(x_a)\n",
        "# create model\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 100, 100, 4)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 100, 100, 16)      592       \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 100, 100, 16)      4768      \n",
            "_________________________________________________________________\n",
            "functional_3 (Functional)    (None, 100, 100, 16)      4768      \n",
            "_________________________________________________________________\n",
            "functional_5 (Functional)    (None, 100, 100, 32)      7632      \n",
            "_________________________________________________________________\n",
            "functional_7 (Functional)    (None, 100, 100, 64)      16208     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              12846080  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10000)             10250000  \n",
            "=================================================================\n",
            "Total params: 23,130,048\n",
            "Trainable params: 23,129,760\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofx0r2YGdlxU"
      },
      "source": [
        "# apply weights of trained local model to global model's residual block\n",
        "res_blocks = {'res_block_16' : res_block_16, 'res_block_16_1' : res_block_16_1,'res_block_32' : res_block_32, 'res_block_64' : res_block_64}\n",
        "\n",
        "res_blocks_path = './res_block_weights/'\n",
        "\n",
        "for key, item in res_blocks.items():\n",
        "  item.load_weights(res_blocks_path + key)\n",
        "  item.trainable = False #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR3JCjSEYNzM"
      },
      "source": [
        "learning_rate = 1e-4\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "model.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAe3iNVFY3qc",
        "outputId": "fd1c9ae5-891b-4110-fb7b-c372f6a8c96c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# data_gen_obj = data_gen(files_train, 64, total_len)\n",
        "history1 = model.fit(train_data, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  2/312 [..............................] - ETA: 1:21 - loss: 9.3543 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0935s vs `on_train_batch_end` time: 0.1933s). Check your callbacks.\n",
            "312/312 [==============================] - 1715s 5s/step - loss: 8.4945 - accuracy: 0.0070\n",
            "Epoch 2/10\n",
            "312/312 [==============================] - 95s 304ms/step - loss: 4.6241 - accuracy: 0.1793\n",
            "Epoch 3/10\n",
            "312/312 [==============================] - 94s 302ms/step - loss: 1.6626 - accuracy: 0.5884\n",
            "Epoch 4/10\n",
            "312/312 [==============================] - 94s 301ms/step - loss: 0.6633 - accuracy: 0.8210\n",
            "Epoch 5/10\n",
            "312/312 [==============================] - 94s 302ms/step - loss: 0.4153 - accuracy: 0.8924\n",
            "Epoch 6/10\n",
            "312/312 [==============================] - 94s 302ms/step - loss: 0.2668 - accuracy: 0.9344\n",
            "Epoch 7/10\n",
            "312/312 [==============================] - 94s 302ms/step - loss: 0.1656 - accuracy: 0.9629\n",
            "Epoch 8/10\n",
            "312/312 [==============================] - 94s 301ms/step - loss: 0.1176 - accuracy: 0.9762\n",
            "Epoch 9/10\n",
            "312/312 [==============================] - 94s 302ms/step - loss: 0.0989 - accuracy: 0.9818\n",
            "Epoch 10/10\n",
            "312/312 [==============================] - 94s 302ms/step - loss: 0.0848 - accuracy: 0.9845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3BAWaadiNK7"
      },
      "source": [
        "# before fine tuning\n",
        "import matplotlib.pyplot as plt\n",
        "history = history1.history\n",
        "plt.plot(history['loss'], 'r')\n",
        "plt.plot(history['val_loss'], 'g')\n",
        "plt.plot(history['out_cropped_accuracy'], 'b')\n",
        "plt.plot(history['val_out_cropped_accuracy'], 'y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBL6Lmthfhwv"
      },
      "source": [
        "# fine tune model, de-freeze res block ex : model.trainable = True\n",
        "for key, item in res_blocks.items():\n",
        "  item.trainable = True "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPzBeUSef5xy"
      },
      "source": [
        "# fine tune phase \n",
        "epochs = 5\n",
        "train_samples = 20000\n",
        "train_files = filelist\n",
        "train_steps_per_epoch = train_samples // batch_size\n",
        "\n",
        "validation_samples = 2000\n",
        "validation_files = filelist[::-1]# get samples from back of file list\n",
        "validation_steps_per_epoch = validation_samples // batch_size\n",
        "\n",
        "train_data = inp_data_generator(train_files, epochs, train_steps_per_epoch, batch_size)\n",
        "validation_data = inp_data_generator(validation_files, epochs + 3, validation_steps_per_epoch, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8zEvh1WgSeE"
      },
      "source": [
        "history2 = model.fit(train_data, validation_data = validation_data, steps_per_epoch=train_steps_per_epoch, validation_steps = validation_steps_per_epoch, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WZ8X-Y3n3bk"
      },
      "source": [
        "# after fine tuning\n",
        "import matplotlib.pyplot as plt\n",
        "history = history2.history\n",
        "plt.plot(history['loss'], 'r')\n",
        "plt.plot(history['val_loss'], 'g')\n",
        "plt.plot(history['out_cropped_accuracy'], 'b')\n",
        "plt.plot(history['val_out_cropped_accuracy'], 'y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8xd3Jsan6ui"
      },
      "source": [
        "# test modle performance on test data\n",
        "from random import sample\n",
        "\n",
        "test_filelist = sample(filelist, 1000) # choose 1000 random files\n",
        "\n",
        "test_data = inp_data_generator(test_filelist, 1, 10, 64)\n",
        "\n",
        "loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy =  model.evaluate(test_data, steps = 10)\n",
        "print('testing model on random data from dataset total loss : %f, touch_loss : %f, cropped_loss = %f, touch_accuracy : %f, cropped_accuracy : %f' % (loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CigvMaeLoFWG"
      },
      "source": [
        "# save model weights for inference\n",
        "model.save_weights(\"global_model_weights\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}