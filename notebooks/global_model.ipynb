{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8LsB1fZiyu9"
   },
   "source": [
    "SETUP ENVIRONMENT\n",
    "\n",
    "change working directory and extract modified svg's if not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2086,
     "status": "ok",
     "timestamp": 1611296587756,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "v__Vb4YjQsJn"
   },
   "outputs": [],
   "source": [
    "# imports \n",
    "import tarfile\n",
    "from os import path, chdir\n",
    "\n",
    "# constants \n",
    "working_directory = \"/content/drive/My Drive/train_global_model/\"\n",
    "\n",
    "# setup environment\n",
    "chdir(working_directory)\n",
    "\n",
    "# extract tar file of svg's\n",
    "fname = './assets/kanji_modified.tar.gz'\n",
    "\n",
    "if not path.isdir('./assets/kanji_modified'):\n",
    "  print('kanji modified svgs not found !, extracting ...')\n",
    "  tar = tarfile.open(fname, \"r:gz\")\n",
    "  tar.extractall(path=\"./assets/\")\n",
    "  tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGrZLmfki-17"
   },
   "source": [
    "STROKE GENERATOR\n",
    "\n",
    "class DataGenerator subclass of keras.utils.sequence, provides data samples neccessary for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8769,
     "status": "ok",
     "timestamp": 1611296594989,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "jdrZfP_iRNbS"
   },
   "outputs": [],
   "source": [
    "from global_strokegenerator import strokeGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, filelist, batch_size, total_samples, data_aug):\n",
    "        self.filelist = filelist\n",
    "        self.batch_size = batch_size\n",
    "        self.total_samples = total_samples\n",
    "        self.sg = strokeGenerator(self.filelist, dataaug=data_aug) # generator which yields x,y\n",
    "        self.data_aug = data_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        # return steps per epoch\n",
    "        return self.total_samples // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_batch = []\n",
    "        out_batch = []\n",
    "        # return ith step batch with len of dimenstion '0' = batch size\n",
    "        for batch in range(batch_size):\n",
    "          inp, out = next(self.sg)\n",
    "          inp_batch.append(inp)\n",
    "          out_batch.append(out) # predict out of 10,000 classes\n",
    "        return np.array(inp_batch), np.array(out_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # get a new generator to ensure same set of samples every epoch\n",
    "        self.sg = strokeGenerator(self.filelist, dataaug=self.data_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCOFqGFNjPJH"
   },
   "source": [
    "INITIALISE DATA GENERATOR\n",
    "\n",
    "data generators for both training and validation are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75321,
     "status": "ok",
     "timestamp": 1611296662621,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "TKC8T4svQVlI",
    "outputId": "3e54f350-8afc-46b5-9876-cb28cdba3147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file count :  11401\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "inp_img_dim = [100, 100, 4]\n",
    "target_img_dim = 10000\n",
    "\n",
    "from os import walk\n",
    "path = \"./assets/kanji_modified/\"\n",
    "_, _, filelist = next(walk(path))\n",
    "print(\"file count : \", len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1449,
     "status": "ok",
     "timestamp": 1611297265118,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "oRY1bFBCW_ED"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "train_samples = 29900\n",
    "train_files = filelist[:10000] # 10000 character for training\n",
    "\n",
    "validation_samples = 10000\n",
    "validation_files = filelist[10000:]\n",
    "\n",
    "train_data = DataGenerator(train_files, batch_size, train_samples, data_aug = True)\n",
    "validation_data = DataGenerator(validation_files, batch_size, validation_samples, data_aug = False) # do not augment validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaNsaTCKjjoc"
   },
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1611296673392,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "s6UBr4rIVx3-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFryk6hcjmdW"
   },
   "source": [
    "RESIDUAL MODULE\n",
    "\n",
    "all four residual blocks are defined, Convolution in each of these blocks have filters [[16, 16], [16, 16], [16, 32], [32, 64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6981,
     "status": "ok",
     "timestamp": 1611296681668,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "n8qbluvyO2Fd"
   },
   "outputs": [],
   "source": [
    "#residual module\n",
    "\n",
    "inp = Input(shape=(100, 100, 16))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x)\n",
    "out = add([x, inp])\n",
    "res_block_16 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "inp = Input(shape=(100, 100, 16))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x)\n",
    "out = add([x, inp])\n",
    "res_block_16_1 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "#residual module\n",
    "inp = Input(shape=(100, 100, 16))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(32, 3,padding=\"same\")(x)\n",
    "if not inp.shape[-1] == 32:\n",
    "  #project with 1x1 convolution\n",
    "  con = Conv2D(32,1)(inp)\n",
    "out = add([x, con])\n",
    "res_block_32 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "#residual module\n",
    "inp = Input(shape=(100, 100, 32))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(64, 3,padding=\"same\")(x)\n",
    "if not inp.shape[-1] == 64:\n",
    "  #project with 1x1 convolution\n",
    "  con = Conv2D(64,1)(inp)\n",
    "out = add([x, con])\n",
    "res_block_64 = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv9_FugskD7O"
   },
   "source": [
    "DEFINE GLOBAL MODEL\n",
    "\n",
    "four residual blocks stacked, output is flatten and fed to Dense layer with 10,000 nodes. intermediate layer with 1024 nodes is defined only to reduce number of parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1307,
     "status": "ok",
     "timestamp": 1611297270586,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "SsKUVc0tSlQm",
    "outputId": "b2103e8d-f504-4d8b-8c2a-b8291311673b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 100, 100, 4)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 100, 100, 16)      592       \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 100, 100, 16)      4768      \n",
      "_________________________________________________________________\n",
      "model_1 (Functional)         (None, 100, 100, 16)      4768      \n",
      "_________________________________________________________________\n",
      "model_2 (Functional)         (None, 100, 100, 32)      7632      \n",
      "_________________________________________________________________\n",
      "model_3 (Functional)         (None, 100, 100, 64)      16208     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              12846080  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10000)             10250000  \n",
      "=================================================================\n",
      "Total params: 23,130,048\n",
      "Trainable params: 23,129,760\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#two block res module\n",
    "inp = Input(shape=(inp_img_dim))\n",
    "x_a = Conv2D(16, 3, padding='same')(inp)\n",
    "x_a = res_block_16(x_a)\n",
    "x_a = res_block_16_1(x_a)\n",
    "x_a = res_block_32(x_a)\n",
    "x_a = res_block_64(x_a)\n",
    "x_a = MaxPooling2D(7)(x_a)\n",
    "x_a = Flatten()(x_a)\n",
    "x_a = Dense(1024, activation='relu')(x_a) # intermediate layer to reduce parameters\n",
    "out = Dense(target_img_dim,activation='softmax')(x_a)\n",
    "# create model\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srs_t49ZkcIh"
   },
   "source": [
    "TRANSFER LEARNING\n",
    "\n",
    "re-use local model residual weights if nessasary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofx0r2YGdlxU"
   },
   "outputs": [],
   "source": [
    "# # apply weights of trained local model to global model's residual block\n",
    "# res_blocks = {'res_block_16' : res_block_16, 'res_block_16_1' : res_block_16_1,'res_block_32' : res_block_32, 'res_block_64' : res_block_64}\n",
    "\n",
    "# res_blocks_path = './res_block_weights/'\n",
    "\n",
    "# for key, item in res_blocks.items():\n",
    "#   item.load_weights(res_blocks_path + key)\n",
    "#   # item.trainable = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY4TCiqokwci"
   },
   "source": [
    "COMPILE MODEL\n",
    "\n",
    "specify appropriate loss and addtional metrics to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2000,
     "status": "ok",
     "timestamp": 1611297274947,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "sR3JCjSEYNzM"
   },
   "outputs": [],
   "source": [
    "# compile model with appropriate loss and specify additional metric ex: accuracy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN8GtZIVk5ex"
   },
   "source": [
    "EARLY STOPPING\n",
    "\n",
    "call-back to monitor validation loss and stop training if loss does not decrease after 2 consecutive epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1682,
     "status": "ok",
     "timestamp": 1611296684499,
     "user": {
      "displayName": "Prajwal T R 1DA17CS114",
      "photoUrl": "",
      "userId": "05160379953557878550"
     },
     "user_tz": -330
    },
    "id": "iHp45lRFZzpI"
   },
   "outputs": [],
   "source": [
    "# callback to monitor val loss\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cy-UOculMRJ"
   },
   "source": [
    "TRAINING GLOBAL MODEL\n",
    "\n",
    "if previously trained global model weights are available, then use it for weight initialization for better start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAe3iNVFY3qc"
   },
   "outputs": [],
   "source": [
    "# use wieghts form previous training \n",
    "# model.load_weights(\"global_model_weights\") , validation_data = validation_data, , validation_steps = validation_samples // batch_size\n",
    "history1 = model.fit(train_data, steps_per_epoch = train_samples // batch_size, epochs = epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ6hyEe-lmlJ"
   },
   "source": [
    "PLOT LEARNING CURVES\n",
    "\n",
    "use history object returned after complete training of model, and plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3BAWaadiNK7"
   },
   "outputs": [],
   "source": [
    "# before fine tuning\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "histry = history1.history\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "epochs = range(1, len(history['loss'])+1)\n",
    "ax.plot(epochs, histry['loss'], 'r', label='train loss')\n",
    "ax.plot(epochs, histry['accuracy'], 'g', label='train accuracy')\n",
    "ax.plot(epochs, histry['val_loss'], 'y', label='val loss')\n",
    "ax.plot(epochs, histry['val_accuracy'], 'b', label='val accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBL6Lmthfhwv"
   },
   "outputs": [],
   "source": [
    "# fine tune model, de-freeze res block ex : model.trainable = True\n",
    "for key, item in res_blocks.items():\n",
    "  item.trainable = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPzBeUSef5xy"
   },
   "outputs": [],
   "source": [
    "# fine tune phase \n",
    "epochs = 5\n",
    "train_samples = 20000\n",
    "train_files = filelist\n",
    "train_steps_per_epoch = train_samples // batch_size\n",
    "\n",
    "validation_samples = 2000\n",
    "validation_files = filelist[::-1]# get samples from back of file list\n",
    "validation_steps_per_epoch = validation_samples // batch_size\n",
    "\n",
    "train_data = inp_data_generator(train_files, epochs, train_steps_per_epoch, batch_size)\n",
    "validation_data = inp_data_generator(validation_files, epochs + 3, validation_steps_per_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8zEvh1WgSeE"
   },
   "outputs": [],
   "source": [
    "history2 = model.fit(train_data, validation_data = validation_data, steps_per_epoch=train_steps_per_epoch, validation_steps = validation_steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WZ8X-Y3n3bk"
   },
   "outputs": [],
   "source": [
    "# after fine tuning\n",
    "import matplotlib.pyplot as plt\n",
    "history = history2.history\n",
    "plt.plot(history['loss'], 'r')\n",
    "plt.plot(history['val_loss'], 'g')\n",
    "plt.plot(history['out_cropped_accuracy'], 'b')\n",
    "plt.plot(history['val_out_cropped_accuracy'], 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8xd3Jsan6ui"
   },
   "outputs": [],
   "source": [
    "# test modle performance on test data\n",
    "from random import sample\n",
    "\n",
    "test_filelist = sample(filelist, 1000) # choose 1000 random files\n",
    "\n",
    "test_data = inp_data_generator(test_filelist, 1, 10, 64)\n",
    "\n",
    "loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy =  model.evaluate(test_data, steps = 10)\n",
    "print('testing model on random data from dataset total loss : %f, touch_loss : %f, cropped_loss = %f, touch_accuracy : %f, cropped_accuracy : %f' % (loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CigvMaeLoFWG"
   },
   "outputs": [],
   "source": [
    "# save model weights for inference\n",
    "model.save_weights(\"save_weights/global_model_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYyVfxMhvXW6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOdyD+a8IfInABgOprtZoo7",
   "collapsed_sections": [],
   "mount_file_id": "1AaIU2iLOccsfVrQUbMla21JWFgq2bKYt",
   "name": "global_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
