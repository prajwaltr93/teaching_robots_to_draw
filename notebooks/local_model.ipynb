{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"local_model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"17Miuapav8h60sj0AqkBtY2rTVIEIew-i","authorship_tag":"ABX9TyPOrH096C6wxLG9HBIODcjg"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n9cjtHPHMzt-"},"source":["Extract Data\n","\n","Extract kanjivg_modified.tar.gz into working directory"]},{"cell_type":"code","metadata":{"id":"rri7GZzmM0HT","executionInfo":{"status":"ok","timestamp":1603694031145,"user_tz":-330,"elapsed":961,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["# imports \n","import tarfile\n","from os import path, chdir\n","\n","# constants \n","working_directory = \"/content/drive/My Drive/train_local_model/\"\n","\n","# setup environment\n","chdir(working_directory)\n","\n","# extract tar file of svg's\n","fname = './assets/kanjivg_modified.tar.gz'\n","\n","if not path.isdir('./assets/kanji_modified'):\n","  print('kanji modified svgs not found !, extracting ...')\n","  tar = tarfile.open(fname, \"r:gz\")\n","  tar.extractall()\n","  tar.close()"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LSXMz5PpNGoj"},"source":["Data Generator \n","\n","Extract Modified svg's and create data batches to feed into the model"]},{"cell_type":"code","metadata":{"id":"GyDB7CMrNUDb","executionInfo":{"status":"ok","timestamp":1603694033769,"user_tz":-330,"elapsed":997,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["from local_strokegenerator import *\n","\n","# data generator\n","\n","def inp_data_generator(filelist, epochs, steps_per_epoch, batch_size):\n","  for epoch in range(epochs):\n","    # get a new generator, same dataset on each epoch\n","    stroke_gen = strokeGenerator(filelist)\n","    for step in range(steps_per_epoch):\n","      # place holder for samples generated every batch\n","      inp_batch = []\n","      ext_batch = []\n","      touch_batch = []\n","      cropped_batch = []  \n","      for batch in range(batch_size):\n","        inp, ext, touch, croppedimg = next(stroke_gen)\n","        inp_batch.append(inp)\n","        ext_batch.append(ext)\n","        touch_batch.append(touch)\n","        cropped_batch.append(np.reshape(croppedimg, 5*5)) # reshape 5 * 5 image to array of length 25 \n","      yield [np.array(inp_batch), np.array(ext_batch)], [np.array(touch_batch), np.array(cropped_batch)] # [x1, x2], [y1, y2]"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qr0ledplzUsz"},"source":["Constants and Magic Numbers"]},{"cell_type":"code","metadata":{"id":"D5c9E-R2vJoH","executionInfo":{"status":"ok","timestamp":1603694036585,"user_tz":-330,"elapsed":1330,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["batch_size = 128\n","inp_img_dim = [100, 100, 3]\n","inp_ext_dim = [3]\n","epochs = 5\n","\n","from os import walk\n","path = \"./assets/kanji_modified/\"\n","_, _, filelist = next(walk(path))\n","\n","train_samples = 250000\n","train_files = filelist\n","train_steps_per_epoch = train_samples // batch_size\n","\n","validation_samples = 20000\n","validation_files = filelist[::-1]# get samples from end of file list\n","validation_steps_per_epoch = validation_samples // batch_size\n","\n","train_data = inp_data_generator(train_files, epochs, train_steps_per_epoch, batch_size)\n","validation_data = inp_data_generator(validation_files, epochs + 3, validation_steps_per_epoch, batch_size)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiVhHolLEloi"},"source":["Residual Block \n","\n","Convolution channels in each of four blocks are [[16,16], [16,32], [32,32],[32,64]]\n"]},{"cell_type":"code","metadata":{"id":"uPf1WzhdHN2D","executionInfo":{"status":"ok","timestamp":1603694040797,"user_tz":-330,"elapsed":2480,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","# from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import add\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","\n","# force graph building by disabling eager execution\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","tf.compat.v1.experimental.output_all_intermediates(True)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ni3UGg541F6u","executionInfo":{"status":"ok","timestamp":1603694041399,"user_tz":-330,"elapsed":2371,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["#residual module\n","\n","# test : 1*1 conv before add layer\n","inp = Input(shape=(100, 100, 16))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x)\n","out = add([x, inp])\n","res_block_16 = Model(inputs=inp, outputs=out)\n","\n","inp = Input(shape=(100, 100, 16))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x)\n","out = add([x, inp])\n","res_block_16_1 = Model(inputs=inp, outputs=out)\n","\n","#residual module\n","inp = Input(shape=(100, 100, 16))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(32, 3,padding=\"same\")(x)\n","if not inp.shape[-1] == 32:\n","  #project with 1x1 convolution\n","  con = Conv2D(32,1)(inp)\n","out = add([x, con])\n","res_block_32 = Model(inputs=inp, outputs=out)\n","\n","#residual module\n","inp = Input(shape=(100, 100, 32))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(64, 3,padding=\"same\")(x)\n","if not inp.shape[-1] == 64:\n","  #project with 1x1 convolution\n","  con = Conv2D(64,1)(inp)\n","out = add([x, con])\n","res_block_64 = Model(inputs=inp, outputs=out)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9WMRTiZ28UO"},"source":["Building Model\n","\n","4 Block Residual block with dynamic tensor extraction \n","\n","two input, two output model (keras functional API)"]},{"cell_type":"code","metadata":{"id":"CQRyVtVR23Oz","executionInfo":{"status":"ok","timestamp":1603694044905,"user_tz":-330,"elapsed":4210,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"6b307c6f-df2a-43d5-bbb8-f0316888ca1b","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# tensor slice \n","def slice_return(x):\n","  return tf.slice(x[0], x[1], [5,5,64])\n","\n","#creating local model\n","inp = Input(shape = inp_img_dim, dtype=tf.float32, name = \"lg_inp\")\n","ext_inp = Input(shape = inp_ext_dim, dtype = tf.int32, name = \"ext_inp\")\n","\n","# initilization layer, helpful in transfer learning, due to different input layers, between global and local models\n","conv = Conv2D(16, 3, padding='same')(inp)\n","#four residual block stacked \n","x_a = res_block_16(conv) \n","x_a = res_block_16_1(x_a) \n","x_a = res_block_32(x_a) \n","x_a = res_block_64(x_a)\n","\n","# now x is 95 * 65 * 54 res encoded tensor, now carry out extraction procedure to enforce localization\n","# batch_size is Dynamic, Unknown or of type None, hence using map_fn to iterate over dimention '0' --> None\n","extracted_tensor = tf.map_fn(slice_return, elems = (x_a, ext_inp), fn_output_signature=tf.float32)\n","\n","x_0 = Flatten()(extracted_tensor) #flatten and feed to dense layer\n","\n","#fully connected layer 1\n","\n","# x1 = Dense(256, activation='relu')(x_0)\n","\n","x1 = Dense(128, activation='relu')(x_0)\n","\n","#fully connected layer 2\n","\n","x2 = Dense(1, activation='sigmoid', name = 'out_touch')(x1)\n","\n","#fully connected layer 3\n","x3 = Dense(25, activation='softmax', name = 'out_cropped')(x1)\n","\n","# x3 = tf.reshape(x3, (5,5)) #output a 5 * 5 image\n","\n","model = Model(inputs= [inp, ext_inp], outputs= [x2,x3])\n","\n","model.summary()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Model: \"functional_9\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","lg_inp (InputLayer)             [(None, 100, 100, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 100, 100, 16) 448         lg_inp[0][0]                     \n","__________________________________________________________________________________________________\n","functional_1 (Functional)       (None, 100, 100, 16) 4768        conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","functional_3 (Functional)       (None, 100, 100, 16) 4768        functional_1[0][0]               \n","__________________________________________________________________________________________________\n","functional_5 (Functional)       (None, 100, 100, 32) 7632        functional_3[0][0]               \n","__________________________________________________________________________________________________\n","functional_7 (Functional)       (None, 100, 100, 64) 16208       functional_5[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_map/Shape (TensorFl [(4,)]               0           functional_7[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_map/strided_slice ( [()]                 0           tf_op_layer_map/Shape[0][0]      \n","__________________________________________________________________________________________________\n","ext_inp (InputLayer)            [(None, 3)]          0                                            \n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayV2_2 [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayUnst [()]                 0           functional_7[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayUnst [()]                 0           ext_inp[0][0]                    \n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while (TensorFl [(), (), (), (), (), 0           tf_op_layer_map/strided_slice[0][\n","                                                                 tf_op_layer_map/TensorArrayV2_2[0\n","                                                                 tf_op_layer_map/strided_slice[0][\n","                                                                 tf_op_layer_map/TensorArrayUnstac\n","                                                                 tf_op_layer_map/TensorArrayUnstac\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/Identity_ [()]                 0           tf_op_layer_map/while[0][3]      \n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayV2St [(None, 5, 5, 64)]   0           tf_op_layer_map/while/Identity_3[\n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 1600)         0           tf_op_layer_map/TensorArrayV2Stac\n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128)          204928      flatten[0][0]                    \n","__________________________________________________________________________________________________\n","out_touch (Dense)               (None, 1)            129         dense[0][0]                      \n","__________________________________________________________________________________________________\n","out_cropped (Dense)             (None, 25)           3225        dense[0][0]                      \n","==================================================================================================\n","Total params: 242,106\n","Trainable params: 241,818\n","Non-trainable params: 288\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5BeRAZ3l3SH9","executionInfo":{"status":"ok","timestamp":1603694048748,"user_tz":-330,"elapsed":1119,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["model.compile(loss=[tf.keras.losses.BinaryCrossentropy(),tf.keras.losses.CategoricalCrossentropy()],\n","              optimizer='adam', metrics = ['accuracy'], loss_weights = [0.2, 1])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgIbzBi-5hyy","executionInfo":{"status":"ok","timestamp":1603697614384,"user_tz":-330,"elapsed":3553574,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"842f3f71-c90c-4898-df74-662fbfbafa3c","colab":{"base_uri":"https://localhost:8080/","height":263}},"source":["history = model.fit(train_data, validation_data = validation_data, validation_steps = validation_steps_per_epoch, steps_per_epoch =  train_steps_per_epoch, epochs = epochs)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1953/1953 [==============================] - ETA: 0s - batch: 976.0000 - size: 128.0000 - loss: 0.3102 - out_touch_loss: 0.0074 - out_cropped_loss: 0.3087 - out_touch_accuracy: 0.9978 - out_cropped_accuracy: 0.8816WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","1953/1953 [==============================] - 711s 364ms/step - batch: 976.0000 - size: 128.0000 - loss: 0.3102 - out_touch_loss: 0.0074 - out_cropped_loss: 0.3087 - out_touch_accuracy: 0.9978 - out_cropped_accuracy: 0.8816 - val_loss: 0.2592 - val_out_touch_loss: 7.2084e-04 - val_out_cropped_loss: 0.2591 - val_out_touch_accuracy: 0.9998 - val_out_cropped_accuracy: 0.8989\n","Epoch 2/5\n","1953/1953 [==============================] - 710s 363ms/step - batch: 976.0000 - size: 128.0000 - loss: 0.2495 - out_touch_loss: 8.4685e-04 - out_cropped_loss: 0.2493 - out_touch_accuracy: 0.9998 - out_cropped_accuracy: 0.8999 - val_loss: 0.2504 - val_out_touch_loss: 4.2369e-04 - val_out_cropped_loss: 0.2504 - val_out_touch_accuracy: 0.9999 - val_out_cropped_accuracy: 0.9006\n","Epoch 3/5\n","1953/1953 [==============================] - 709s 363ms/step - batch: 976.0000 - size: 128.0000 - loss: 0.2394 - out_touch_loss: 5.3362e-04 - out_cropped_loss: 0.2393 - out_touch_accuracy: 0.9998 - out_cropped_accuracy: 0.9033 - val_loss: 0.2487 - val_out_touch_loss: 3.4243e-04 - val_out_cropped_loss: 0.2486 - val_out_touch_accuracy: 0.9999 - val_out_cropped_accuracy: 0.9019\n","Epoch 4/5\n","1953/1953 [==============================] - 706s 361ms/step - batch: 976.0000 - size: 128.0000 - loss: 0.2324 - out_touch_loss: 7.3450e-04 - out_cropped_loss: 0.2322 - out_touch_accuracy: 0.9998 - out_cropped_accuracy: 0.9059 - val_loss: 0.2404 - val_out_touch_loss: 8.6924e-05 - val_out_cropped_loss: 0.2404 - val_out_touch_accuracy: 1.0000 - val_out_cropped_accuracy: 0.9048\n","Epoch 5/5\n","1953/1953 [==============================] - 709s 363ms/step - batch: 976.0000 - size: 128.0000 - loss: 0.2247 - out_touch_loss: 4.4936e-04 - out_cropped_loss: 0.2246 - out_touch_accuracy: 0.9999 - out_cropped_accuracy: 0.9087 - val_loss: 0.2396 - val_out_touch_loss: 9.3320e-05 - val_out_cropped_loss: 0.2396 - val_out_touch_accuracy: 1.0000 - val_out_cropped_accuracy: 0.9050\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wo1gfXfI7UVV","executionInfo":{"status":"ok","timestamp":1603697688355,"user_tz":-330,"elapsed":1154,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"c5f1740b-cb8a-45d4-f67c-ce012ed6d720","colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["import matplotlib.pyplot as plt\n","# history = history.history\n","plt.plot(history['loss'], 'r', label=\"train_loss\")\n","plt.plot(history['val_loss'], 'g', label='validation_loss')\n","plt.plot(history['out_cropped_accuracy'], 'b', label='train_cropped_accuracy')\n","plt.plot(history['val_out_cropped_accuracy'], 'y', label = 'validation accuracy')\n","plt.savefig('local_model_firstattempt', fmt='PNG')"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"qc16K0X-7of3","executionInfo":{"status":"ok","timestamp":1603697754857,"user_tz":-330,"elapsed":5564,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"5262c76f-a121-4a69-f0e7-8012ddd4139e","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# test model loss and accuracy\n","from random import sample\n","\n","test_filelist = sample(filelist, 1000) # choose 1000 random files\n","\n","test_data = inp_data_generator(test_filelist, 1, 10, 64)\n","\n","loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy =  model.evaluate(test_data, steps = 10)\n","print('testing model on random data from dataset total loss : %f, touch_loss : %f, cropped_loss = %f, touch_accuracy : %f, cropped_accuracy : %f' % (loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["testing model on random data from dataset total loss : 0.186636, touch_loss : 0.000036, cropped_loss = 0.186629, touch_accuracy : 1.000000, cropped_accuracy : 0.926562\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9dH7Udt5bQcZ","executionInfo":{"status":"ok","timestamp":1603697778448,"user_tz":-330,"elapsed":3409,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["# save residiual block weights for transfer learning\n","res_blocks = {'res_block_16' : res_block_16, 'res_block_16_1' : res_block_16_1, 'res_block_32' : res_block_32, 'res_block_64' : res_block_64}\n","\n","res_blocks_path = './res_block_weights/'\n","for key, item in res_blocks.items():\n","  item.save_weights(res_blocks_path + key)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwItrWPNXKRm","executionInfo":{"status":"ok","timestamp":1603697853740,"user_tz":-330,"elapsed":3269,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["# save whole model weights for inference\n","model.save_weights(\"local_model_trained_1\")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSkKeRW01xNn"},"source":[""],"execution_count":null,"outputs":[]}]}