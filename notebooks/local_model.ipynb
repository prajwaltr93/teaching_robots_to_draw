{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9cjtHPHMzt-"
   },
   "source": [
    "Extract Data\n",
    "\n",
    "Extract kanjivg_modified.tar.gz into working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rri7GZzmM0HT"
   },
   "outputs": [],
   "source": [
    "# imports \n",
    "import tarfile\n",
    "from os import path, chdir\n",
    "\n",
    "# constants \n",
    "working_directory = \"/content/drive/My Drive/train_local_model/\"\n",
    "\n",
    "# setup environment\n",
    "chdir(working_directory)\n",
    "\n",
    "# extract tar file of svg's\n",
    "fname = './assets/kanji_modified.tar.gz'\n",
    "\n",
    "if not path.isdir('./assets/kanji_modified'):\n",
    "  print('kanji modified svgs not found !, extracting ...')\n",
    "  tar = tarfile.open(fname, \"r:gz\")\n",
    "  tar.extractall(path=\"./assets/\")\n",
    "  tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSXMz5PpNGoj"
   },
   "source": [
    "Data Generator \n",
    "\n",
    "Extract Modified svg's and create data batches to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyDB7CMrNUDb"
   },
   "outputs": [],
   "source": [
    "from local_strokegenerator import *\n",
    "\n",
    "# data generator\n",
    "\n",
    "def inp_data_generator(filelist, epochs, steps_per_epoch, batch_size):\n",
    "  for epoch in range(epochs):\n",
    "    # get a new generator, same dataset on each epoch\n",
    "    stroke_gen = strokeGenerator(filelist)\n",
    "    for step in range(steps_per_epoch):\n",
    "      # place holder for samples generated every batch\n",
    "      inp_batch = []\n",
    "      ext_batch = []\n",
    "      touch_batch = []\n",
    "      cropped_batch = []  \n",
    "      for batch in range(batch_size):\n",
    "        inp, ext, touch, croppedimg = next(stroke_gen)\n",
    "        inp_batch.append(inp)\n",
    "        ext_batch.append(ext)\n",
    "        touch_batch.append(touch)\n",
    "        cropped_batch.append(np.reshape(croppedimg, 5*5)) # reshape 5 * 5 image to array of length 25 \n",
    "      yield [np.array(inp_batch), np.array(ext_batch)], [np.array(touch_batch), np.array(cropped_batch)] # [x1, x2], [y1, y2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr0ledplzUsz"
   },
   "source": [
    "Constants and Magic Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5c9E-R2vJoH",
    "outputId": "be2e898e-80c2-462f-ca08-00b984b67fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file count :  11401\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "from os import walk\n",
    "path = \"./assets/kanji_modified/\"\n",
    "_, _, filelist = next(walk(path))\n",
    "\n",
    "print(\"file count : \", len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv0msPCdSaNP"
   },
   "outputs": [],
   "source": [
    "train_samples = 1700000\n",
    "train_files = filelist\n",
    "train_steps_per_epoch = train_samples // batch_size\n",
    "\n",
    "validation_samples = 40000\n",
    "validation_files = filelist[::-1]# get samples from end of file list\n",
    "validation_steps_per_epoch = validation_samples // batch_size\n",
    "\n",
    "train_data = inp_data_generator(train_files, epochs, train_steps_per_epoch, batch_size)\n",
    "validation_data = inp_data_generator(validation_files, epochs + 3, validation_steps_per_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiVhHolLEloi"
   },
   "source": [
    "Residual Block \n",
    "\n",
    "Convolution channels in each of four blocks are [[16,16], [16,32], [32,32],[32,64]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPf1WzhdHN2D"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "# from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# force graph building by disabling eager execution\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ni3UGg541F6u"
   },
   "outputs": [],
   "source": [
    "#residual module\n",
    "inp_img_dim = [100, 100, 3]\n",
    "inp_ext_dim = [3]\n",
    "# test : 1*1 conv before add layer\n",
    "inp = Input(shape=(100, 100, 16))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x)\n",
    "out = add([x, inp])\n",
    "res_block_16 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "inp = Input(shape=(100, 100, 16))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x)\n",
    "out = add([x, inp])\n",
    "res_block_16_1 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "#residual module\n",
    "inp = Input(shape=(100, 100, 16))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(32, 3,padding=\"same\")(x)\n",
    "if not inp.shape[-1] == 32:\n",
    "  #project with 1x1 convolution\n",
    "  con = Conv2D(32,1)(inp)\n",
    "out = add([x, con])\n",
    "res_block_32 = Model(inputs=inp, outputs=out)\n",
    "\n",
    "#residual module\n",
    "inp = Input(shape=(100, 100, 32))\n",
    "x = BatchNormalization()(inp)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(16, 3,padding=\"same\")(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(64, 3,padding=\"same\")(x)\n",
    "if not inp.shape[-1] == 64:\n",
    "  #project with 1x1 convolution\n",
    "  con = Conv2D(64,1)(inp)\n",
    "out = add([x, con])\n",
    "res_block_64 = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9WMRTiZ28UO"
   },
   "source": [
    "Building Model\n",
    "\n",
    "4 Block Residual block with dynamic tensor extraction \n",
    "\n",
    "two input, two output model (keras functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQRyVtVR23Oz",
    "outputId": "fa96cd71-c099-445a-df56-5be668bfdf15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lg_inp (InputLayer)             [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 100, 100, 16) 448         lg_inp[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 100, 100, 16) 4768        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "functional_3 (Functional)       (None, 100, 100, 16) 4768        functional_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "functional_5 (Functional)       (None, 100, 100, 32) 7632        functional_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "functional_7 (Functional)       (None, 100, 100, 64) 16208       functional_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/Shape (TensorFl [(4,)]               0           functional_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/strided_slice ( [()]                 0           tf_op_layer_map/Shape[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ext_inp (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/TensorArrayV2_2 [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/TensorArrayUnst [()]                 0           functional_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/TensorArrayUnst [()]                 0           ext_inp[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while (TensorFl [(), (), (), (), (), 0           tf_op_layer_map/strided_slice[0][\n",
      "                                                                 tf_op_layer_map/TensorArrayV2_2[0\n",
      "                                                                 tf_op_layer_map/strided_slice[0][\n",
      "                                                                 tf_op_layer_map/TensorArrayUnstac\n",
      "                                                                 tf_op_layer_map/TensorArrayUnstac\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "                                                                 tf_op_layer_map/while/EmptyTensor\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/while/Identity_ [()]                 0           tf_op_layer_map/while[0][3]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_map/TensorArrayV2St [(None, 5, 5, 64)]   0           tf_op_layer_map/while/Identity_3[\n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1600)         0           tf_op_layer_map/TensorArrayV2Stac\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          204928      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_touch (Dense)               (None, 1)            129         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "out_cropped (Dense)             (None, 25)           3225        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 242,106\n",
      "Trainable params: 241,818\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# tensor slice \n",
    "def slice_return(x):\n",
    "  return tf.slice(x[0], x[1], [5,5,64])\n",
    "\n",
    "#creating local model\n",
    "inp = Input(shape = inp_img_dim, dtype=tf.float32, name = \"lg_inp\")\n",
    "ext_inp = Input(shape = inp_ext_dim, dtype = tf.int32, name = \"ext_inp\")\n",
    "\n",
    "# initilization layer, helpful in transfer learning, due to different input layers, between global and local models\n",
    "conv = Conv2D(16, 3, padding='same')(inp)\n",
    "#four residual block stacked \n",
    "x_a = res_block_16(conv) \n",
    "x_a = res_block_16_1(x_a) \n",
    "x_a = res_block_32(x_a) \n",
    "x_a = res_block_64(x_a)\n",
    "\n",
    "# now x is 95 * 65 * 54 res encoded tensor, now carry out extraction procedure to enforce localization\n",
    "# batch_size is Dynamic, Unknown or of type None, hence using map_fn to iterate over dimention '0' --> None\n",
    "extracted_tensor = tf.map_fn(slice_return, elems = (x_a, ext_inp), fn_output_signature=tf.float32)\n",
    "\n",
    "x_0 = Flatten()(extracted_tensor) #flatten and feed to dense layer\n",
    "\n",
    "#fully connected layer 1\n",
    "\n",
    "# x1 = Dense(256, activation='relu')(x_0)\n",
    "\n",
    "x1 = Dense(128, activation='relu')(x_0)\n",
    "\n",
    "#fully connected layer 2\n",
    "\n",
    "x2 = Dense(1, activation='sigmoid', name = 'out_touch')(x1)\n",
    "\n",
    "#fully connected layer 3\n",
    "x3 = Dense(25, activation='softmax', name = 'out_cropped')(x1)\n",
    "\n",
    "# x3 = tf.reshape(x3, (5,5)) #output a 5 * 5 image\n",
    "\n",
    "model = Model(inputs= [inp, ext_inp], outputs= [x2,x3])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BeRAZ3l3SH9"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=[tf.keras.losses.BinaryCrossentropy(),tf.keras.losses.CategoricalCrossentropy()],\n",
    "              optimizer='adam', metrics = ['accuracy'], loss_weights = [0.2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngG5RNcqDHCR"
   },
   "outputs": [],
   "source": [
    "# call backs to ensure optimal training\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_out_cropped_loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgIbzBi-5hyy",
    "outputId": "764d3dfe-2e54-44b9-ef23-9256521401c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13281/13281 [==============================] - ETA: 0s - batch: 6640.0000 - size: 128.0000 - loss: 0.1208 - out_touch_loss: 0.0016 - out_cropped_loss: 0.1205 - out_touch_accuracy: 0.9996 - out_cropped_accuracy: 0.9565WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "13281/13281 [==============================] - 6182s 465ms/step - batch: 6640.0000 - size: 128.0000 - loss: 0.1208 - out_touch_loss: 0.0016 - out_cropped_loss: 0.1205 - out_touch_accuracy: 0.9996 - out_cropped_accuracy: 0.9565 - val_loss: 0.1075 - val_out_touch_loss: 1.6939e-04 - val_out_cropped_loss: 0.1075 - val_out_touch_accuracy: 0.9999 - val_out_cropped_accuracy: 0.9597\n",
      "Epoch 2/20\n",
      "13281/13281 [==============================] - 6135s 462ms/step - batch: 6640.0000 - size: 128.0000 - loss: 0.0991 - out_touch_loss: 1.9663e-04 - out_cropped_loss: 0.0990 - out_touch_accuracy: 0.9999 - out_cropped_accuracy: 0.9628 - val_loss: 0.0966 - val_out_touch_loss: 2.9285e-04 - val_out_cropped_loss: 0.0965 - val_out_touch_accuracy: 0.9999 - val_out_cropped_accuracy: 0.9650\n",
      "Epoch 3/20\n",
      "13281/13281 [==============================] - 6138s 462ms/step - batch: 6640.0000 - size: 128.0000 - loss: 0.0940 - out_touch_loss: 1.1095e-04 - out_cropped_loss: 0.0940 - out_touch_accuracy: 1.0000 - out_cropped_accuracy: 0.9644 - val_loss: 0.0976 - val_out_touch_loss: 2.3771e-05 - val_out_cropped_loss: 0.0976 - val_out_touch_accuracy: 1.0000 - val_out_cropped_accuracy: 0.9642\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, validation_data = validation_data, validation_steps = validation_steps_per_epoch, steps_per_epoch =  train_steps_per_epoch, epochs = epochs, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "Wo1gfXfI7UVV",
    "outputId": "aedcaa2f-0395-4369-e8ac-4acb8b48e186"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history = history.history\n",
    "plt.plot(history['loss'], 'r', label=\"train_loss\")\n",
    "plt.plot(history['val_loss'], 'g', label='validation_loss')\n",
    "plt.plot(history['out_cropped_accuracy'], 'b', label='train_cropped_accuracy')\n",
    "plt.plot(history['val_out_cropped_accuracy'], 'y', label = 'validation accuracy')\n",
    "plt.savefig('local_model_firstattempt', fmt='PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc16K0X-7of3",
    "outputId": "6f21a5d0-9ffc-4c9d-ba77-fb4eaa46a108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model on random data from dataset total loss : 0.087605, touch_loss : 0.000001, cropped_loss = 0.087605, touch_accuracy : 1.000000, cropped_accuracy : 0.970312\n"
     ]
    }
   ],
   "source": [
    "# test model loss and accuracy\n",
    "from random import sample\n",
    "\n",
    "test_filelist = sample(filelist[::-1], 1000) # choose 1000 random files\n",
    "\n",
    "test_data = inp_data_generator(test_filelist, 1, 10, 64)\n",
    "\n",
    "loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy =  model.evaluate(test_data, steps = 10)\n",
    "print('testing model on random data from dataset total loss : %f, touch_loss : %f, cropped_loss = %f, touch_accuracy : %f, cropped_accuracy : %f' % (loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dH7Udt5bQcZ"
   },
   "outputs": [],
   "source": [
    "# save residiual block weights for transfer learning\n",
    "res_blocks = {'res_block_16' : res_block_16, 'res_block_16_1' : res_block_16_1, 'res_block_32' : res_block_32, 'res_block_64' : res_block_64}\n",
    "\n",
    "res_blocks_path = './res_block_weights/'\n",
    "for key, item in res_blocks.items():\n",
    "  item.save_weights(res_blocks_path + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwItrWPNXKRm"
   },
   "outputs": [],
   "source": [
    "# save whole model weights for inference\n",
    "model.save_weights(\"saved_weights/local_model_trained_17mil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSkKeRW01xNn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "local_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
