{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"local_model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"17Miuapav8h60sj0AqkBtY2rTVIEIew-i","authorship_tag":"ABX9TyNEAB6Dr+598dYvEFqp+C9o"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n9cjtHPHMzt-"},"source":["Extract Data\n","\n","Extract kanjivg_modified.tar.gz into working directory"]},{"cell_type":"code","metadata":{"id":"rri7GZzmM0HT","executionInfo":{"status":"ok","timestamp":1603968548416,"user_tz":-330,"elapsed":66998,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"a629e909-236e-4730-a58c-2135354b2e03","colab":{"base_uri":"https://localhost:8080/"}},"source":["# imports \n","import tarfile\n","from os import path, chdir\n","\n","# constants \n","working_directory = \"/content/drive/My Drive/train_local_model/\"\n","\n","# setup environment\n","chdir(working_directory)\n","\n","# extract tar file of svg's\n","fname = './assets/kanji_modified.tar.gz'\n","\n","if not path.isdir('./assets/kanji_modified'):\n","  print('kanji modified svgs not found !, extracting ...')\n","  tar = tarfile.open(fname, \"r:gz\")\n","  tar.extractall(path=\"./assets/\")\n","  tar.close()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["kanji modified svgs not found !, extracting ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LSXMz5PpNGoj"},"source":["Data Generator \n","\n","Extract Modified svg's and create data batches to feed into the model"]},{"cell_type":"code","metadata":{"id":"GyDB7CMrNUDb","executionInfo":{"status":"ok","timestamp":1603968577500,"user_tz":-330,"elapsed":4477,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["from local_strokegenerator import *\n","\n","# data generator\n","\n","def inp_data_generator(filelist, epochs, steps_per_epoch, batch_size):\n","  for epoch in range(epochs):\n","    # get a new generator, same dataset on each epoch\n","    stroke_gen = strokeGenerator(filelist)\n","    for step in range(steps_per_epoch):\n","      # place holder for samples generated every batch\n","      inp_batch = []\n","      ext_batch = []\n","      touch_batch = []\n","      cropped_batch = []  \n","      for batch in range(batch_size):\n","        inp, ext, touch, croppedimg = next(stroke_gen)\n","        inp_batch.append(inp)\n","        ext_batch.append(ext)\n","        touch_batch.append(touch)\n","        cropped_batch.append(np.reshape(croppedimg, 5*5)) # reshape 5 * 5 image to array of length 25 \n","      yield [np.array(inp_batch), np.array(ext_batch)], [np.array(touch_batch), np.array(cropped_batch)] # [x1, x2], [y1, y2]"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qr0ledplzUsz"},"source":["Constants and Magic Numbers"]},{"cell_type":"code","metadata":{"id":"D5c9E-R2vJoH","executionInfo":{"status":"ok","timestamp":1603968580375,"user_tz":-330,"elapsed":1243,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["batch_size = 128\n","inp_img_dim = [100, 100, 3]\n","inp_ext_dim = [3]\n","epochs = 5\n","\n","from os import walk\n","path = \"./assets/kanji_modified/\"\n","_, _, filelist = next(walk(path))\n","\n","train_samples = 300000\n","train_files = filelist\n","train_steps_per_epoch = train_samples // batch_size\n","\n","validation_samples = 20000\n","validation_files = filelist[::-1]# get samples from end of file list\n","validation_steps_per_epoch = validation_samples // batch_size\n","\n","train_data = inp_data_generator(train_files, epochs, train_steps_per_epoch, batch_size)\n","validation_data = inp_data_generator(validation_files, epochs + 3, validation_steps_per_epoch, batch_size)\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiVhHolLEloi"},"source":["Residual Block \n","\n","Convolution channels in each of four blocks are [[16,16], [16,32], [32,32],[32,64]]\n"]},{"cell_type":"code","metadata":{"id":"uPf1WzhdHN2D","executionInfo":{"status":"ok","timestamp":1603968586658,"user_tz":-330,"elapsed":2509,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","# from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import add\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","\n","# force graph building by disabling eager execution\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","tf.compat.v1.experimental.output_all_intermediates(True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ni3UGg541F6u","executionInfo":{"status":"ok","timestamp":1603968593838,"user_tz":-330,"elapsed":3411,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["#residual module\n","\n","# test : 1*1 conv before add layer\n","inp = Input(shape=(100, 100, 16))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x)\n","out = add([x, inp])\n","res_block_16 = Model(inputs=inp, outputs=out)\n","\n","inp = Input(shape=(100, 100, 16))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x)\n","out = add([x, inp])\n","res_block_16_1 = Model(inputs=inp, outputs=out)\n","\n","#residual module\n","inp = Input(shape=(100, 100, 16))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(32, 3,padding=\"same\")(x)\n","if not inp.shape[-1] == 32:\n","  #project with 1x1 convolution\n","  con = Conv2D(32,1)(inp)\n","out = add([x, con])\n","res_block_32 = Model(inputs=inp, outputs=out)\n","\n","#residual module\n","inp = Input(shape=(100, 100, 32))\n","x = BatchNormalization()(inp)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(16, 3,padding=\"same\")(x) \n","x = BatchNormalization()(x)\n","x = Activation(\"relu\")(x)\n","x = Conv2D(64, 3,padding=\"same\")(x)\n","if not inp.shape[-1] == 64:\n","  #project with 1x1 convolution\n","  con = Conv2D(64,1)(inp)\n","out = add([x, con])\n","res_block_64 = Model(inputs=inp, outputs=out)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9WMRTiZ28UO"},"source":["Building Model\n","\n","4 Block Residual block with dynamic tensor extraction \n","\n","two input, two output model (keras functional API)"]},{"cell_type":"code","metadata":{"id":"CQRyVtVR23Oz","executionInfo":{"status":"ok","timestamp":1603968606305,"user_tz":-330,"elapsed":7845,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"f7e2df88-2032-480e-dc26-7d936bba8ddb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# tensor slice \n","def slice_return(x):\n","  return tf.slice(x[0], x[1], [5,5,64])\n","\n","#creating local model\n","inp = Input(shape = inp_img_dim, dtype=tf.float32, name = \"lg_inp\")\n","ext_inp = Input(shape = inp_ext_dim, dtype = tf.int32, name = \"ext_inp\")\n","\n","# initilization layer, helpful in transfer learning, due to different input layers, between global and local models\n","conv = Conv2D(16, 3, padding='same')(inp)\n","#four residual block stacked \n","x_a = res_block_16(conv) \n","x_a = res_block_16_1(x_a) \n","x_a = res_block_32(x_a) \n","x_a = res_block_64(x_a)\n","\n","# now x is 95 * 65 * 54 res encoded tensor, now carry out extraction procedure to enforce localization\n","# batch_size is Dynamic, Unknown or of type None, hence using map_fn to iterate over dimention '0' --> None\n","extracted_tensor = tf.map_fn(slice_return, elems = (x_a, ext_inp), fn_output_signature=tf.float32)\n","\n","x_0 = Flatten()(extracted_tensor) #flatten and feed to dense layer\n","\n","#fully connected layer 1\n","\n","# x1 = Dense(256, activation='relu')(x_0)\n","\n","x1 = Dense(128, activation='relu')(x_0)\n","\n","#fully connected layer 2\n","\n","x2 = Dense(1, activation='sigmoid', name = 'out_touch')(x1)\n","\n","#fully connected layer 3\n","x3 = Dense(25, activation='softmax', name = 'out_cropped')(x1)\n","\n","# x3 = tf.reshape(x3, (5,5)) #output a 5 * 5 image\n","\n","model = Model(inputs= [inp, ext_inp], outputs= [x2,x3])\n","\n","model.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Model: \"functional_9\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","lg_inp (InputLayer)             [(None, 100, 100, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 100, 100, 16) 448         lg_inp[0][0]                     \n","__________________________________________________________________________________________________\n","functional_1 (Functional)       (None, 100, 100, 16) 4768        conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","functional_3 (Functional)       (None, 100, 100, 16) 4768        functional_1[0][0]               \n","__________________________________________________________________________________________________\n","functional_5 (Functional)       (None, 100, 100, 32) 7632        functional_3[0][0]               \n","__________________________________________________________________________________________________\n","functional_7 (Functional)       (None, 100, 100, 64) 16208       functional_5[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_map/Shape (TensorFl [(4,)]               0           functional_7[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_map/strided_slice ( [()]                 0           tf_op_layer_map/Shape[0][0]      \n","__________________________________________________________________________________________________\n","ext_inp (InputLayer)            [(None, 3)]          0                                            \n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayV2_2 [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayUnst [()]                 0           functional_7[0][0]               \n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayUnst [()]                 0           ext_inp[0][0]                    \n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/EmptyTens [()]                 0           tf_op_layer_map/strided_slice[0][\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while (TensorFl [(), (), (), (), (), 0           tf_op_layer_map/strided_slice[0][\n","                                                                 tf_op_layer_map/TensorArrayV2_2[0\n","                                                                 tf_op_layer_map/strided_slice[0][\n","                                                                 tf_op_layer_map/TensorArrayUnstac\n","                                                                 tf_op_layer_map/TensorArrayUnstac\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","                                                                 tf_op_layer_map/while/EmptyTensor\n","__________________________________________________________________________________________________\n","tf_op_layer_map/while/Identity_ [()]                 0           tf_op_layer_map/while[0][3]      \n","__________________________________________________________________________________________________\n","tf_op_layer_map/TensorArrayV2St [(None, 5, 5, 64)]   0           tf_op_layer_map/while/Identity_3[\n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 1600)         0           tf_op_layer_map/TensorArrayV2Stac\n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128)          204928      flatten[0][0]                    \n","__________________________________________________________________________________________________\n","out_touch (Dense)               (None, 1)            129         dense[0][0]                      \n","__________________________________________________________________________________________________\n","out_cropped (Dense)             (None, 25)           3225        dense[0][0]                      \n","==================================================================================================\n","Total params: 242,106\n","Trainable params: 241,818\n","Non-trainable params: 288\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5BeRAZ3l3SH9","executionInfo":{"status":"ok","timestamp":1603968614172,"user_tz":-330,"elapsed":1177,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["model.compile(loss=[tf.keras.losses.BinaryCrossentropy(),tf.keras.losses.CategoricalCrossentropy()],\n","              optimizer='adam', metrics = ['accuracy'], loss_weights = [0.2, 1])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgIbzBi-5hyy","executionInfo":{"status":"ok","timestamp":1603971477383,"user_tz":-330,"elapsed":2864148,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"32e8769b-07c8-4c2a-94ba-25dfcbe6ff85","colab":{"base_uri":"https://localhost:8080/"}},"source":["history = model.fit(train_data, validation_data = validation_data, validation_steps = validation_steps_per_epoch, steps_per_epoch =  train_steps_per_epoch, epochs = epochs)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","2343/2343 [==============================] - ETA: 0s - batch: 1171.0000 - size: 128.0000 - loss: 0.1578 - out_touch_loss: 0.0046 - out_cropped_loss: 0.1569 - out_touch_accuracy: 0.9988 - out_cropped_accuracy: 0.9468WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","2343/2343 [==============================] - 572s 244ms/step - batch: 1171.0000 - size: 128.0000 - loss: 0.1578 - out_touch_loss: 0.0046 - out_cropped_loss: 0.1569 - out_touch_accuracy: 0.9988 - out_cropped_accuracy: 0.9468 - val_loss: 0.1279 - val_out_touch_loss: 8.4195e-04 - val_out_cropped_loss: 0.1277 - val_out_touch_accuracy: 0.9997 - val_out_cropped_accuracy: 0.9524\n","Epoch 2/5\n","2343/2343 [==============================] - 570s 243ms/step - batch: 1171.0000 - size: 128.0000 - loss: 0.1188 - out_touch_loss: 8.8470e-04 - out_cropped_loss: 0.1187 - out_touch_accuracy: 0.9998 - out_cropped_accuracy: 0.9566 - val_loss: 0.1172 - val_out_touch_loss: 4.0198e-04 - val_out_cropped_loss: 0.1171 - val_out_touch_accuracy: 0.9998 - val_out_cropped_accuracy: 0.9569\n","Epoch 3/5\n","2343/2343 [==============================] - 572s 244ms/step - batch: 1171.0000 - size: 128.0000 - loss: 0.1112 - out_touch_loss: 6.7875e-04 - out_cropped_loss: 0.1111 - out_touch_accuracy: 0.9998 - out_cropped_accuracy: 0.9588 - val_loss: 0.1116 - val_out_touch_loss: 9.8700e-05 - val_out_cropped_loss: 0.1116 - val_out_touch_accuracy: 1.0000 - val_out_cropped_accuracy: 0.9587\n","Epoch 4/5\n","2343/2343 [==============================] - 571s 244ms/step - batch: 1171.0000 - size: 128.0000 - loss: 0.1068 - out_touch_loss: 4.4316e-04 - out_cropped_loss: 0.1067 - out_touch_accuracy: 0.9998 - out_cropped_accuracy: 0.9603 - val_loss: 0.1077 - val_out_touch_loss: 1.2405e-04 - val_out_cropped_loss: 0.1076 - val_out_touch_accuracy: 0.9999 - val_out_cropped_accuracy: 0.9591\n","Epoch 5/5\n","2343/2343 [==============================] - 567s 242ms/step - batch: 1171.0000 - size: 128.0000 - loss: 0.1039 - out_touch_loss: 3.8666e-04 - out_cropped_loss: 0.1039 - out_touch_accuracy: 0.9999 - out_cropped_accuracy: 0.9613 - val_loss: 0.1073 - val_out_touch_loss: 6.7845e-05 - val_out_cropped_loss: 0.1073 - val_out_touch_accuracy: 0.9999 - val_out_cropped_accuracy: 0.9602\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wo1gfXfI7UVV","executionInfo":{"status":"ok","timestamp":1603971500384,"user_tz":-330,"elapsed":1719,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"7c1cf973-5454-4d87-9943-f65c391f5cc1","colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["import matplotlib.pyplot as plt\n","history = history.history\n","plt.plot(history['loss'], 'r', label=\"train_loss\")\n","plt.plot(history['val_loss'], 'g', label='validation_loss')\n","plt.plot(history['out_cropped_accuracy'], 'b', label='train_cropped_accuracy')\n","plt.plot(history['val_out_cropped_accuracy'], 'y', label = 'validation accuracy')\n","plt.savefig('local_model_firstattempt', fmt='PNG')"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"qc16K0X-7of3","executionInfo":{"status":"ok","timestamp":1603971509348,"user_tz":-330,"elapsed":1671,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}},"outputId":"c97e82d0-2f82-4e57-dd3f-e297396c55a1","colab":{"base_uri":"https://localhost:8080/"}},"source":["# test model loss and accuracy\n","from random import sample\n","\n","test_filelist = sample(filelist, 1000) # choose 1000 random files\n","\n","test_data = inp_data_generator(test_filelist, 1, 10, 64)\n","\n","loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy =  model.evaluate(test_data, steps = 10)\n","print('testing model on random data from dataset total loss : %f, touch_loss : %f, cropped_loss = %f, touch_accuracy : %f, cropped_accuracy : %f' % (loss, touch_loss, cropped_loss, touch_accuracy, cropped_accuracy))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["testing model on random data from dataset total loss : 0.097201, touch_loss : 0.000015, cropped_loss = 0.097198, touch_accuracy : 1.000000, cropped_accuracy : 0.965625\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9dH7Udt5bQcZ","executionInfo":{"status":"ok","timestamp":1603971521774,"user_tz":-330,"elapsed":4472,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["# save residiual block weights for transfer learning\n","res_blocks = {'res_block_16' : res_block_16, 'res_block_16_1' : res_block_16_1, 'res_block_32' : res_block_32, 'res_block_64' : res_block_64}\n","\n","res_blocks_path = './res_block_weights/'\n","for key, item in res_blocks.items():\n","  item.save_weights(res_blocks_path + key)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwItrWPNXKRm","executionInfo":{"status":"ok","timestamp":1603971522281,"user_tz":-330,"elapsed":4809,"user":{"displayName":"Prajwal T R 1DA17CS114","photoUrl":"","userId":"05160379953557878550"}}},"source":["# save whole model weights for inference\n","model.save_weights(\"local_model_trained_1\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSkKeRW01xNn"},"source":[""],"execution_count":null,"outputs":[]}]}